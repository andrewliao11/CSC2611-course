
\documentclass[a4paper]{article}

%\usepackage[pages=all, color=black, position={current page.south}, placement=bottom, scale=1, opacity=1, vshift=5mm]{background}

\usepackage[margin=1in]{geometry} % full-width

% AMS Packages
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{booktabs}

\usepackage[
backend=biber,
style=alphabetic,
sorting=ynt
]{biblatex}
\addbibresource{ref.bib}

% Unicode
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\newcommand{\AL}[1]{{\color{blue}{[Andrew: #1]}}}

\hypersetup{
	unicode,
%	colorlinks,
%	breaklinks,
%	urlcolor=cyan, 
%	linkcolor=blue, 
	pdfauthor={Author One, Author Two, Author Three},
	pdftitle={A simple article template},
	pdfsubject={A simple article template},
	pdfkeywords={article, template, simple},
	pdfproducer={LaTeX},
	pdfcreator={pdflatex}
}

% Vietnamese
%\usepackage{vntex}

% Natbib
%\usepackage[sort&compress,numbers,square]{natbib}
%\bibliographystyle{mplainnat}

% Theorem, Lemma, etc
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{claim}{Claim}[theorem]
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{hypothesis}[theorem]{Hypothesis}
\newtheorem{assumption}[theorem]{Assumption}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{criterion}[theorem]{Criterion}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{principle}[theorem]{Principle}

\usepackage{graphicx, color}
\graphicspath{{fig/}}

%\usepackage[linesnumbered,ruled,vlined,commentsnumbered]{algorithm2e} % use algorithm2e for typesetting algorithms
\usepackage{algorithm, algpseudocode} % use algorithm and algorithmicx for typesetting algorithms
\usepackage{mathrsfs} % for \mathscr command

\usepackage{lipsum}

% Author info
\title{CSC2611 Project Proposal}
\author{Yuan-Hong Liao}

\date{
	Computer Science at the University of Toronto \\ \texttt{andrew@cs.toronto.edu}
}

\begin{document}
\maketitle

\section{Proposal}

\subsection{Introduction}

The meanings of words continuously change over time, reflecting complicated processes in language and society. 
Studying these types of changes in meaning enables researchers to learn more about human language and to extract temporal-dependent data from texts~\cite{kutuzov-etal-2018-diachronic}.
The surge of large corpora and computation enables us to detect the diachronic semantic shift in a data-driven manner.
In this project, we want to improve the performance of lexical semantic change detection (LSC) in an unsupervised manner~\cite{schlechtweg-etal-2020-semeval}.


\subsection{Problem Formulation}
Given two time-specific corpora $C^1, C^2$ and $N$ target words $\{w_i\}_{i=1:N}$.
We denote the set of sentences containing the target word $w$ in corpus $C^1$ as $S_w^1 \subseteq C^1$.
We perform unsupervised predictions on which words lost or gained sense(s) between $C^1$ and $C^2$ and which ones did not.
We formulate the problem as a binary classification problem and report the accuracy of each word.


\subsection{Proposed Approach: Influence Function}

We follow the intuition that if a word shares similar \textit{semantic meanings} between two different corpora, the sentences containing the word also have similar \textit{influences} to a model.
On the contrary, if a word loses or gains any sense between two corpora, the sentences containing the word impact a model differently.

To measure the influences on a model, let us consider a prediction problem that takes a sentence as input.
For a loss function $\mathcal{L}(s, \theta)$, let $\sum_{s} \mathcal{L}(s, \theta)$ be the empirical risk and $\hat{\theta} = \arg\min_{\theta \in \Theta} \sum_{s} \mathcal{L}(s, \theta)$ be the empirical risk minimizer.
We define the influence of $S_w^1$ to the difference in model $\Delta_{S_w^1} = \hat{\theta}_{-S_w^1} - \hat{\theta}$, where $\hat{\theta}_{-S_w^1} = \arg\min_{\theta \in \Theta} \sum_{s \not\in S_w^1} \mathcal{L}(s, \theta)$.
If the target word is stable across two corpora, we expect $\Delta_{S_w^1}$ and $\Delta_{S_w^2}$ are similar, and vice versa.

However, it is prohibitive to train two neural networks to detect the LSC of a single target word. 
Inspired from the inflence function for neural networks~\cite{classic-influences, pmlr-v70-koh17a}, we locally approximate $\Delta_{S_w^1}$ as:



\begin{align}
\begin{split}
	\Delta_{S_w^1} \approx \frac{-1}{|\{ C^1, C^2 \}|} H^{-1}_{\hat{\theta}} \sum_{s \in S_w^1} \nabla_\theta \mathcal{L}(s, \theta)
	\label{eq.influence}
\end{split}
\end{align}

\noindent where $H_{\hat{\theta}} \coloneqq \frac{1}{|\{ C^1, C^2 \}|} \sum_{s \in \{ C^1, C^2 \}} \nabla^2 \mathcal{L}(s, \hat{\theta})$ is the Hessian and is positive definite (PD) by assumption. 
For the proof of Eq.~\ref{eq.influence}, please refere to ~\cite{pmlr-v70-koh17a}.

To perform LSC detection, we use the L2 norm between $\Delta_{S_w^1}$ and $\Delta_{S_w^2}$. If the difference is larger than a threshold $\tau$, we predict that the target word $w$ gained/lost sense(s) between $C^1$ and $C^2$.


\noindent\textbf{Dataset.} We use the dataset provided in ~\cite{schlechtweg-etal-2020-semeval} to benchmark the detection performance.


\clearpage
\printbibliography

\end{document}